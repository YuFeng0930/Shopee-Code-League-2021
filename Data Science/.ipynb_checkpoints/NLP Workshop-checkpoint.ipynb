{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk, spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm #en_core_web_md, en_core_web_lg, en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generic Data cleaning steps to take:*\n",
    "\n",
    "**1. Converting text to lower case** \n",
    "* if your data is case insensitive, you'll need to consider your usecase as well\n",
    "* some exceptions, if there're acronyms, which may lose their meaning if converted to word, e.g. in german language, MIT (university) vs mit (german for with)\n",
    "\n",
    "**2. Removing punctuations**\n",
    "* exceptions, twitter data, where punctuations may be good in showing sentiment\n",
    "\n",
    "**3. Removing any numerical values**\n",
    "\n",
    "**4. Tokenize text**\n",
    "* different ways to tokenize the text\n",
    "\n",
    "**5. Remove stop words**\n",
    "* stop words can be altered to be usecase specific, \n",
    "* e.g. adding restaurant name to list of stopwords for restaurant review, as the people will usually give the restaurant name in the review\n",
    "* e.g. twitter you may want to add words like 'RT' 'Retweet' etc for tweets\n",
    "\n",
    "\n",
    "**6. Stemming / lemmatization** \n",
    "* different variation of words due to suffix, but the meaning of the word is the same, use of the word is identical, to prevent duplication of words being stored and improve efficiency\n",
    "* lemmatization result is a proper word, unlike stemming\n",
    "\n",
    "\n",
    "**7. Parts of speech tagging**\n",
    "* good for identifying, summarising, to get e.g. retrieval of nouns from large pieces of texts (NNP)\n",
    "\n",
    "**8. Create bi-grams or tri-grams**\n",
    "\n",
    "**9. Deal with typos and spelling mistakes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = nlp('''Hello Mr. Tan, this is the first sentence. \n",
    "           This is the second sentence. \n",
    "           Notice, the abbreviations, e.g. Mr. Goh, Ms. Ng, do not get tokenised wrongly.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in para.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nlp('I went to the nearby grocers to buy ten oranges and 5 apples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sentence:\n",
    "    print(word, word.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.text for token in sentence if not token.is_stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords.add('retweet') # RT,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords.update({'zooloo','zzz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_stopwords.remove('single') #discard\n",
    "#all_stopwords.difference_update({'multiple','to','remove'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('She was better than the best in atheletics. She was a great runner. Running was her passion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing stemming -- > NLTK natural language toolkit for python, porter stemmer and snowball stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in para:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nlp('He left his keys in his left pocket.')\n",
    "\n",
    "for token in sentence:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(\"I went out with Jane to the Apple store in Orchard to buy an iphone. It cost me $800. After that I stopped for an apple pie.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
    "\n",
    "\n",
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n",
    "test.tsv contains just phrases. You must assign a sentiment label to each phrase.\n",
    "The sentiment labels are:\n",
    "\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_count = df.groupby('Sentiment').count()\n",
    "\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['Phrase'])\n",
    "plt.xlabel('Review Sentiments')\n",
    "plt.ylabel('Number of Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "\n",
    "text_counts= cv.fit_transform(df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, df['Sentiment'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "text_tf = tf.fit_transform(df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, df['Sentiment'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **TextBlob Module:** Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels.\n",
    "2. **Sentiment Labels:** Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these.\n",
    "   * **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
    "   * **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n",
    "\n",
    "For more info on how TextBlob coded up its [sentiment function](https://planspace.org/20150607-textblob_sentiment/).\n",
    "\n",
    "Let's take a look at the sentiment of the various transcripts, both overall and throughout the comedy routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacytextblob\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "spacy_text_blob = SpacyTextBlob()\n",
    "nlp.add_pipe(spacy_text_blob)\n",
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.sentiment.polarity      # Polarity: -0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.sentiment.subjectivity  # Sujectivity: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.sentiment.assessments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lyrics(url):\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, \"html.parser\")\n",
    "    lyrics = html.find(\"pre\", class_=\"lyric-body\").get_text()\n",
    "    print(url)\n",
    "    #print(lyrics)\n",
    "    return lyrics.replace(\"\\n\",\" \")\n",
    "\n",
    "\n",
    "links = ['https://www.lyrics.com/lyric/36863481/Justin+Bieber/Yummy',\n",
    "     'https://www.lyrics.com/lyric/35362456/Ed+Sheeran/Castle+on+the+Hill',\n",
    "     'https://www.lyrics.com/lyric/35342586/Taylor+Swift/22',\n",
    "     'https://www.lyrics.com/lyric/36147543/Kygo/Happy+Now',\n",
    "     'https://www.lyrics.com/sublyric/58125/Lauv/Superhero',\n",
    "     'https://www.lyrics.com/lyric/30514737/Fix+You',\n",
    "     'https://www.lyrics.com/lyric/32981724/One+Direction/Perfect',\n",
    "     'https://www.lyrics.com/lyric/36489666/Bahari/Crashing',\n",
    "     'https://www.lyrics.com/lyric/33787626/ROZES/Matches',\n",
    "     'https://www.lyrics.com/lyric/36341880/Maroon+5/She+Will+Be+Loved',\n",
    "     'https://www.lyrics.com/lyric/25306933/Queen/Dont+Stop+Me+Now',\n",
    "     'https://www.lyrics.com/lyric/31781320/Eric+Clapton/Tears+In+Heaven']\n",
    "\n",
    "\n",
    "lyrics = [scrape_lyrics(link) for link in links]\n",
    "\n",
    "artists = ['JustinBieber', 'EdSheeran', 'TaylorSwift', 'Kygo', 'Lauv', 'Coldplay', 'OneDirection','Bahari','Rozes','Maroon5', 'Queen', 'EricClapton']\n",
    "\n",
    "#fun fact: queen dont stop me now is apparently the happiest song, and eric clapton is supposedly a sad song\n",
    "# https://www.indy100.com/article/dont-stop-me-now-is-the-happiest-song-in-the-world-according-to-a-neuroscientist-7318321\n",
    "#but there's more to the what affects the sentiment of the song, not just the lyrics, e.g. tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Lyrics':lyrics}, index=artists)\n",
    "\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "df['polarity'] = df['Lyrics'].apply(pol)\n",
    "df['subjectivity'] = df['Lyrics'].apply(sub)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for artist in df.index:\n",
    "    x = df.polarity.loc[artist]\n",
    "    y = df.subjectivity.loc[artist]\n",
    "    plt.scatter(x, y, color='green')\n",
    "    plt.text(x+.001, y+.001, artist, fontsize=10)\n",
    "    plt.xlim(-.7, .7) \n",
    "    plt.ylim(0,1) \n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('<-- Negative ---------- Positive -->', fontsize=15)\n",
    "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
